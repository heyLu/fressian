[
	{
		"id": "7f9d3d6ac6d15b9424c3d9bc4fa0de81",
		"title": "datomic notes",
		"content": "- \"Writing Datomic in Clojure\" [video/slides](http://www.infoq.com/presentations/Datomic)\r\n    - has quite a few internal details, watch it if you want to learn more\r\n    - queries are answered from a merge of in-memory and on-disk data\r\n        - makes persistent on-disk index easier (\"less garbage collection\")\r\n        - everything is still made durable immediately, but not necessarily in the index\r\n            - it's (at least) in the log\r\n\r\nthe various resources about datomic often mention somewhat cryptic (to me) terms, let's start collecting them, maybe written down they will become more meaningful:\r\n\r\n- covering indices (the index has the data, e.g. 3 indices -\u003e data is stored 3 times, the most important benefit is probably performance)\r\n- \"wide branching factor\" (probably *very* wide, i.e. hundreds, maybe even thousands of child nodes)\r\n- \"find any segment with 1-2 storage lookups\" (related to the branching factor, but probably also the result of *other things*)\r\n- the live index is a \"merge of on-disk and in-memory data\"\r\n\r\nterms:\r\n\r\n- datom: entity, attribute, value, transaction, (added?)\r\n    - attributes (also values?) are (sometimes?) mapped to numbers on disk/in the index, likely for (storage) efficiency reasons\r\n- index: ?\r\n    - eavt, aevt, ...\r\n    - from the \"unnofficial guide\":\r\n\r\n        \u003e B-tree-like structures: sorted, immutable, persistent, 1,000+ branching factor, custom sort order, fast lookup and range scans, able to be efficiently merged (details unknown). Index trees are shallow, no more than three level deep: root node, directories and segments as leafs.\r\n        - what are \"directories\"?\r\n    - three parts: history (outdated/retracted datoms, durable), current (latest assertions only (gc because of this?), durable) and in-memory (merged, accumulates novelty)\r\n        - new peers build in-memory index from the current index and the transactions in the log from that point onwards\r\n    - \"pluggable comparators\"\r\n- partition: helps group relevant data closely together in storage/indices\r\n    - \"big pre-allocated entity id ranges\" (2\u003csup\u003e42\u003c/sup\u003e)\r\n- entity id\r\n    - somewhat time based (high bits?)\r\n        - semi-sequential: \"uuid whose most significant 32 bits are currentTimeMillies rounded to seconds\"\r\n    - chosen/generated so that \"related\" data is stored together\r\n        - e.g. same transaction -\u003e closer together, same partition -\u003e closer together\r\n        - \"data from the same transaction is stored togeher\"\r\n- segment: some amount (~50kb, 1000-20000, [source](http://docs.datomic.com/capacity.html#sec-6)) of datoms\r\n    - fressian encoded, gzipped, [heyLu/fressian](https://github.com/heyLu/fressian) can read those segments\r\n    - they are [garbage collected](http://docs.datomic.com/capacity.html#sec-7) when no longer referenced\r\n    - **immutable**, i.e. the *entire* database is an immutable value\r\n\r\nquestions:\r\n\r\n- how are segments segmented?\r\n    - some ideas: number of datoms (1000-10000?)\r\n- how is an index/tree built from segments?\r\n- maybe i should implement some other persistent data structures before trying to figure out datomic?\r\n\r\nnotes:\r\n\r\n- i don't need the \"peer\" part, i'm primarily interested in the persistent db\r\n    - the goal is something like sqlite, but as a persistent db\r\n\r\nresources\r\n\r\n- datomic docs:\r\n    - [indexes](http://docs.datomic.com/indexes.html)\r\n- the BigTable paper influential for merging\r\n- [Unofficial guide to Datomic internals](http://tonsky.me/blog/unofficial-guide-to-datomic-internals/)\r\n- datascript:\r\n    - `btset.cljs` (maybe this is how datomic works internally? or at least very similar?)\r\n    - [A shallow dive into DataScript internals](http://tonsky.me/blog/datascript-internals/)",
		"created": "2015-03-14T21:40:13.62299275+01:00"
	},
	{
		"id": "818ff28b50576536a788bca50366091a",
		"title": "Next steps",
		"content": "- more tests\r\n    - `*_ARRAY`\r\n    - `FLOAT`, `DOUBLE`, `DOUBLE_0`, `DOUBLE_1`\r\n    - for the caches (probably the most tricky)\r\n    - tests should be split up, e.g. `TestReadInt`, `TestReadInt`, `TestReadList`, `TestRead...`\r\n- make it usable\r\n    - `main` should become just `fressian`, maybe in `encoding/fressian`?\r\n    - the pretty-printing part should become a new tool, maybe `fsn pp`?\r\n        - and it would just use the `fressian` package (would require thinking about the public API)\r\n    - document how to use it\r\n- the future\r\n    - writing!\r\n        - maybe write a `gol` backend for fun?\r\n    - extensible handlers, maybe just mirror what `Datomic/fressian` does\r\n    - maybe benchmark it? (not sure against what, would require having some data to use and a benchmark suite for `Datomic/fressian`)\r\n- the far future\r\n    - i also want an `edn` reader (and writer, just for fun)\r\n    - coincidentally, datomic queries are also `edn` data...\r\n    - maybe rename this `clog`? (negative connotations, though.) maybe `clogs`? (\"Holzpantoffel\", not exactly fancy, but kind of cute and indicates that this has multiple things. (`fressian`, `edn`, ...))\r\n\r\nAt this point, I think that we have the world's second most advanced fressian reader? That's... interesting. :)",
		"created": "2015-03-14T14:46:34.959662666+01:00"
	}
]